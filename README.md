## Workflow:

### Step 1: Submission Collection

After downloading dataset (e.g. [CodeHunt](https://github.com/microsoft/Code-Hunt)), we need to run a script `collect.py` to gather student submissions that satisfy certain criteria. Note that `collect.py` in this repo is specifically designed for the structure of CodeHunt dataset. If you are working on a different dataset, you may need to modify the existing script.

The script should complete the following tasks:

- Organize student submissions in a human-friendly way
- Avoid filename collision
- Filter submissions that do not satisfy some rules, such as submissions that cannot be built or in a language that we do not support
- (Optional) Convert the code to the format that our symbolic execution engine (e.g. Pex) accepts. For example, you can add some automated tool to convert submissions written in a different language to C#. The `collect.py` contains an example function that convert Java to C# by using [Sharpen](https://github.com/mono/sharpen).

### Step 2: Test Case Generation

Our approach relies on high quality test cases. Because student submissions are typically small, it is feasible to generate test cases to reach 100% branch coverage for each submission.

For dataset that comes with built-in test cases that are constructed by the instructor, we can simply measure the quality of those test cases and reuse them if they are good enough. Another approach is using automated tool to generate test cases. Such tool usually belongs to two big categories:

#### Symbolic Execution Engine

- Pex (C#)
- [KLEE (C/C++)](https://github.com/klee/klee)
- [angr (x86 binary)](https://github.com/angr/angr)
- [Manticore (x86 binary)](https://github.com/trailofbits/manticore)

#### Fuzzer

- [AFL](https://github.com/google/AFL)
- [libFuzzer](https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md)

For Pex, you can use `runpex.py` to invoke Pex. And, all parameterized unit test templates are placed under `Templates/PUT`.

### Step 3: Path-Condition (PC) & Symbolic Return-Value (SRV) Generation

After generating enough test cases from previous step, we need to using symbolic execution engine to collect path-condition and symbolic return-value that are associated with a test case.

Most of modern dynamic symbolic execution engines support **seeding**. It starts exploration from a given input. Such functionality can be used to collect PC & SRV by using a test case as a seed.

For Pex, `runseed.py` reads test cases and generates PC&SRV for each submission.

### Step 4: Parsing (Optional)

To check equivalency of different PCs & SRVs, we use Z3 SMT solver. For symbolic execution engines that support save path conditions as SMT-LIB2 format or any other formats that are readable for Z3, we can skip this step.

Because Pex is developed as early as 2010, it does not support any formats that are acceptable to Z3. A possible workaround is configuring Pex to output PC & SRV as a conjunction of C# boolean sub-expressions. Then we use `lex_yacc.py` to parse C# expressions and generate corresponding Z3 model.

`lex_yacc.py` only implements a small subset of C# grammar. It returns a Z3 model and a failed string for each expression. The failed string contains sub-expressions that failed to parse.

Example:

```c#
int[] a != null && a[0] > 10 && a[0] < 50 // Path condition
```

`lex_yacc.py` will output:

```python
[And(GT(ELEM(a, 0), 10), LT(ELEM(a, 0), 50)), "int[] a != null"] # Z3 model, failed string
```

### Step 5: Equivalence Checking & Clustering

Two submissions are considered equivalent if for every test input:

- Z3 model of A == Z3 model of B (done by Z3)
- Failed string of A == Failed string of B (Only for Pex)

And two submission are clustered together if they are equivalent.

#### Use Z3 to check equivalency

```
Check_SAT(model_A != model_B)
```

If Z3 can find a solution for the expression above, then it means these two models are not equivalent.

This step is done by `cluster.py`

## Extensions

### Support More C# Grammars

- Extend token definitions in `SimpleLexer` class
- Extend C# grammar from `SimpleParser` class
- Related resources: https://www.dabeaz.com/ply/

### Model Function Calls

Some PC & SRV generated by Pex contain function calls (e.g. abs). `lex_yacc.py` provides an API to register function hooks to mock some function calls.

For example, function `abs` can be modeled as:

```python
def abs(x):
    return z3.If(x > 0, x, -x)
```

### Support A Different Tool / Algorithm

If you want to modify the framework for a different clustering algorithm, you may need to modify `TestCase` and `Submission` class in `cluster.py` to change the way that two submissions are checked.

If you want to add support for a different tool, you may need to create a different script to replace `runpex.py` and `runseed.py`.

## Original README

### Setup:

You'll need to add Pex (`C:\Program Files\Microsoft Pex\bin`), MSBuild (`C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin`), and z3 (`C:\z3-download-location`) to your PATH environmental variable. You'll also need to create a new environmental variable called PYTHONPATH and its value will be (`C:\z3-download-location\bin\python`).

After you get Python set up, you'll need to install: pip. Then use pip to install beautifulsoup4 and pycparser.

You'll need to download and install: z3 (pip install z3-solver).

You'll also need to download JetBrain's [dupFinder](https://www.jetbrains.com/help/resharper/dupFinder.html) CLI tool.

### Running scripts:

Run the scripts in this order to run our tool on a given dataset:

1. `collect.py`
2. `runpex.py`
3. `union_tests.py`
4. `runseed.py`
5. `cluster.py`

To run the JetBrain's dupFinder script on a given dataset:

1. `dupFinder.py`

### Description of the scripts:

`config` just contains all constants (e.g., directories and strings). You may need to update the paths to properly use the tool on your computer.

`collect` copies the submissions from the CodeHunt dataset into your project directory. Use the `-w` (winning) or `-f` (failing) flags to collect only the winning or failing submissions, respectively.

Example of how to run:
`py .\collect.py 2 5` <-- Default
`py .\collect.py -w 2 5` <-- Copy the winning (correct) submissions only

`runpex` runs Pex on all submissions and saves the _unique_, concrete inputs and PCs for all of them.

Example of how to run:
`py .\runpex.py 2 5` <-- Winning submissions only
`py .\runpex.py -j 4 2 5` <-- Winning submissions only w/ 4 parallel threads

`union_tests` prints all _unique_, concrete tests produced after executing run_pex.py.

Example of how to run:
`py .\union_tests.py 2 5` <-- Sector 2, level 5
`py .\union_tests.py 2 5 -n 10` <-- Sector 2, level 5, print only 10 random concrete tests

`runseed` feeds only the _unique_, concrete inputs (aka seeds) to all submissions and runs Pex on them.

Example of how to run:
`py .\runseed.py -d CodeHunt 2 5` <-- Run only on the winning submissions directory of CodeHunt dataset
`py .\runseed.py -d CodeHunt -j 4 2 5` <-- Winning submissions only w/ 4 parallel threads on CodeHunt dataset

`cluster` groups all of the submissions into clusters, based on comparing their Z3-evaluated PCs across _all_ unique, concrete inputs. More concretely, two submissions that share the same PCs over all concrete inputs are clustered together.

Example of how to run:

`py .\cluster.py -d Pex4Fun 2 5 > clusterLog.txt` <-- Sector 2, level 5 (winning only), log output to text file
`py .\cluster.py 9 1 -a` <-- Sector 9, level 1 (algorithms dataset)

---

`dupFinder` runs JetBrain's dupFinder is an AST-based duplicate code detection tool that runs on a given dataset.

Example of how to run:

`py .\dupFinder.py -d CodeHunt "<absolute path to root directory containing all evaluation subjects>"` <-- Runs on CodeHunt dataset
`py .\dupFinder.py -d Pex4Fun -su "<absolute path to sub-directory containing subset of evaluation subjects>"` <-- Runs only on specified sub-directory within Pex4Fun dataset
